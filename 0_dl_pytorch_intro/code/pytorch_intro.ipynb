{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgMAwGVbjQ4L"
      },
      "source": [
        "# PyTorch Basics: Introduction to Deep Learning\n",
        "\n",
        "Today we will learn how to work with PyTorch, one of the most popular deep learning frameworks (libraries). We will see how can we do the same things as in NumPy and more.\n",
        "\n",
        "[How to install](https://pytorch.org/)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch pandas matplotlib seaborn scikit-learn ipywidgets"
      ],
      "metadata": {
        "id": "ukFaO74ejSHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f_vV2i3jQ4M"
      },
      "source": [
        "## 1. Introduction to PyTorch\n",
        "\n",
        "PyTorch is an open-source machine learning library developed by Facebook (now Meta). It's known for its dynamic computational graphs and Python-first approach, making it popular for research and production.\n",
        "\n",
        "### Key Features:\n",
        "- **Dynamic Computational Graphs**: Graphs are built on-the-fly (sounds abstract but we'll discuss)\n",
        "- **Pythonic**: Natural Python syntax and control flow\n",
        "- **GPU Support**: Seamless CPU/GPU computation\n",
        "- **Rich Ecosystem**: Extensive libraries for computer vision, NLP, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CEoz0VDjQ4N"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "SEED = 42\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Check PyTorch version and CUDA availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "def set_plot_params():\n",
        "    plt.style.use(\"seaborn-v0_8-paper\")\n",
        "    sns.set_context(\"paper\", font_scale=1.5)\n",
        "    sns.set_style(\n",
        "        \"ticks\",\n",
        "        {\n",
        "            \"axes.grid\": True,\n",
        "            \"grid.linestyle\": \"--\",\n",
        "            \"grid.alpha\": 0.6,\n",
        "            \"axes.spines.right\": False,\n",
        "            \"axes.spines.top\": False,\n",
        "            \"font.family\": \"serif\",\n",
        "            \"axes.labelpad\": 10,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    colors = [\n",
        "        \"#0173B2\",\n",
        "        \"#DE8F05\",\n",
        "        \"#029E73\",\n",
        "        \"#D55E00\",\n",
        "        \"#CC78BC\",\n",
        "        \"#CA9161\",\n",
        "        \"#FBAFE4\",\n",
        "    ]\n",
        "    sns.set_palette(colors)\n",
        "\n",
        "set_plot_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKHQXDi4jQ4O"
      },
      "source": [
        "## 2. Tensors: The Foundation of PyTorch\n",
        "\n",
        "Tensors are the fundamental data structure in PyTorch, similar to NumPy arrays but with additional features for deep learning.\n",
        "\n",
        "### Key Concepts:\n",
        "- **Tensors** are multi-dimensional arrays\n",
        "- **Shape** defines the dimensions\n",
        "- **Data Type** (dtype) specifies the data type\n",
        "- **Device** (CPU/GPU) determines where computation happens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjNRA_r3jQ4O"
      },
      "source": [
        "### All dtypes\n",
        "\n",
        "- `torch.HalfTensor`      # 16 bits, floating point\n",
        "- `torch.FloatTensor`     # 32 bits, floating point\n",
        "- `torch.DoubleTensor`    # 64 bits, floating point\n",
        "\n",
        "- `torch.ShortTensor`     # 16 bits, integer, signed\n",
        "- `torch.IntTensor`       # 32 bits, integer, signed\n",
        "- `torch.LongTensor`      # 64 bits, integer, signed\n",
        "\n",
        "- `torch.CharTensor`      # 8 bits, integer, signed\n",
        "- `torch.ByteTensor`      # 8 bits, integer, unsigned\n",
        "\n",
        "We will use `torch.FloatTensor()` and `torch.IntTensor()` most of the time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRTt02AXjQ4O"
      },
      "source": [
        "### Creating tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoMsxhiUjQ4O"
      },
      "outputs": [],
      "source": [
        "# From Python lists\n",
        "x = torch.tensor([1, 2, 3, 4])\n",
        "print(f\"From list: {x}, shape: {x.shape}, dtype: {x.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AijKuYTcjQ4P"
      },
      "outputs": [],
      "source": [
        "# From NumPy arrays\n",
        "arr = np.array([[1, 2], [3, 4]])\n",
        "y = torch.from_numpy(arr)\n",
        "print(f\"From NumPy: \\n{y}\\nshape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWxN4POGjQ4P"
      },
      "source": [
        "Good to know when converting data from `numpy` to `torch`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gA982upfjQ4P"
      },
      "outputs": [],
      "source": [
        "y -= y\n",
        "y, arr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKlka2e7jQ4P"
      },
      "outputs": [],
      "source": [
        "# Zeros and ones\n",
        "zeros = torch.zeros(2, 3)\n",
        "ones = torch.ones(2, 3)\n",
        "print(f\"Zeros: \\n{zeros}\")\n",
        "print(f\"Ones: \\n{ones}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STovztBjjQ4P"
      },
      "outputs": [],
      "source": [
        "# Random tensors\n",
        "random_tensor = torch.rand(3, 4)\n",
        "print(f\"Random tensor: \\n{random_tensor}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHCM5e3yjQ4Q"
      },
      "outputs": [],
      "source": [
        "# Specifying data type\n",
        "float_tensor = torch.tensor([1, 2, 3], dtype=torch.float16)\n",
        "print(f\"Float tensor: {float_tensor}, dtype: {float_tensor.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QL-MT0KMjQ4Q"
      },
      "outputs": [],
      "source": [
        "# Creating tenors from various distributions\n",
        "x = torch.randn((2, 3)) # Normal(0, 1) with shape (2, 3)\n",
        "print(f\"x ~ N(0, 1)\\n {x}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn4TGiR8jQ4Q"
      },
      "source": [
        "Most of the methods (functions) in PyTorch have their analog with `_`, e.g. `method` and `method_`. The first one creates a new object and the second changes existing one.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOFG9WmejQ4Q"
      },
      "outputs": [],
      "source": [
        "# Fills x with values from discrite uniform distribution\n",
        "x.random_(0, 10)\n",
        "print(f\"x ~ Ud[0, 10)\\n {x}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnFhC3oojQ4Q"
      },
      "outputs": [],
      "source": [
        "# Fills x with values from uniform distribution\n",
        "x.uniform_(0, 1)\n",
        "print(f\"x ~ U[0, 1]\\n {x}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16yIUQ-mjQ4Q"
      },
      "outputs": [],
      "source": [
        "# Bernoulli with parameter p\n",
        "x.bernoulli_(p=0.5)\n",
        "print((f\"x ~ Bernoulli(0.5)\\n {x}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfikKCTFjQ4Q"
      },
      "source": [
        "### Functions and tensor operations in Torch\n",
        "\n",
        "A lot of function from `numpy` have their pair in `PyTorch`! We just need to remember `numpy` ðŸ˜¨ ðŸ˜° ðŸ˜¥ ðŸ˜“ ðŸ¤—\n",
        "\n",
        "[click here](https://github.com/torch/torch7/wiki/Torch-for-Numpy-users)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p6yuIrajQ4R"
      },
      "source": [
        "#### Basic operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmP_nWfYjQ4R"
      },
      "outputs": [],
      "source": [
        "a = torch.tensor([1, 2, 3])\n",
        "b = torch.tensor([4, 5, 6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIwTov2hjQ4R"
      },
      "outputs": [],
      "source": [
        "# Basic arithmetic\n",
        "print(f\"Addition: {a + b}\")\n",
        "print(f\"Multiplication: {a * b}\")\n",
        "print(f\"Matrix multiplication: {torch.matmul(a, b)}\")\n",
        "print(f\"Matrix multiplication: {torch.matmul(a, b)}\")\n",
        "print(f\"Matrix multiplication @: {a @ b}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4tdjmmVjQ4R"
      },
      "outputs": [],
      "source": [
        "# Reshaping\n",
        "c = torch.arange(12)\n",
        "print(f\"Original: {c}\")\n",
        "print(f\"Reshaped to 3x4: \\n{c.reshape(3, 4)}\")\n",
        "print(f\"Reshaped to 3x4: \\n{c.view(3, 4)}\")\n",
        "print(f\"Reshaped to -1(everything else)x12: \\n{c.reshape(-1, 12)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rdyaodhjQ4R"
      },
      "outputs": [],
      "source": [
        "# Slicing and indexing\n",
        "matrix = torch.rand(4, 4)\n",
        "print(f\"Matrix: \\n{matrix}\")\n",
        "print(f\"First row: {matrix[0]}\")\n",
        "print(f\"First column: {matrix[:, 0]}\")\n",
        "print(f\"Submatrix (rows 1-2, cols 1-2): \\n{matrix[1:3, 1:3]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iysw9j8RjQ4R"
      },
      "outputs": [],
      "source": [
        "# Change dtype\n",
        "print(f\"Dtype of a: {a.dtype}\")\n",
        "print(f\"Cast a to float64: {a.to(torch.float64)}\")\n",
        "print(f\"Cast b to the same dtype as a: {b.type_as(a.to(torch.float64))}\")\n",
        "print(f\".type_as create a new tensor, the old one stays unchganged: {b.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIuhisrHjQ4R"
      },
      "outputs": [],
      "source": [
        "# Applying common funstion element-wise\n",
        "print(f\"sin(a): {torch.sin(a)}\")\n",
        "print(f\"exp(a): {torch.exp(a)}\")\n",
        "print(f\"sigmoid(a): {torch.sigmoid(a)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CJEnxOSjQ4R"
      },
      "source": [
        "#### Aggregation and working with axes\n",
        "\n",
        "`sum`, `mean`, `max`, `min`, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Zylw1WAjQ4R"
      },
      "outputs": [],
      "source": [
        "a = torch.FloatTensor([[-0.5, 0.5, 0], [-10, -20, -30], [100, 200, 300]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k5Gf6iejQ4R"
      },
      "outputs": [],
      "source": [
        "a.sum() # tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ke8-7Sb8jQ4S"
      },
      "outputs": [],
      "source": [
        "a.sum().item() # Python float"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9I1jM1RjQ4S"
      },
      "source": [
        "Operations with tensors return tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Asbligt8jQ4S"
      },
      "outputs": [],
      "source": [
        "# Compare float with float tensor\n",
        "a.sum().item() == a.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7HLKcQPjQ4S"
      },
      "outputs": [],
      "source": [
        "# Not in this case\n",
        "a.sum().item() is a.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p_MEm7yjQ4S"
      },
      "outputs": [],
      "source": [
        "a.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkAwG2H2jQ4S"
      },
      "source": [
        "We can apply methods along a specified axis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCjVZQP1jQ4S"
      },
      "outputs": [],
      "source": [
        "a.sum(dim=0), a.sum(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD1kQqsWjQ4S"
      },
      "outputs": [],
      "source": [
        "max_v, max_i = a.max(dim=0)\n",
        "print(f\"Max values along 0 axis: {max_v}\")\n",
        "print(f\"Indexes of max values: {max_i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJGnh0S-jQ4S"
      },
      "source": [
        "A simple trick to add a new axis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufmpvoBLjQ4S"
      },
      "outputs": [],
      "source": [
        "a.shape, a[:, None, :].shape, a[None, :, :].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00cK9u2BjQ4T"
      },
      "source": [
        "It is hard sometimes to understand which axis you should use, but there is a simple rule: **operations are applyed along the axis which will disappear.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdxBf65JjQ4T"
      },
      "source": [
        "## 3. Automatic Differentiation (Autograd)\n",
        "\n",
        "Autograd is PyTorch's automatic differentiation engine. It computes gradients automatically, which is essential for training neural networks.\n",
        "\n",
        "### Key Concepts:\n",
        "- **requires_grad=True**: Tells PyTorch to track gradients\n",
        "- **backward()**: Computes gradients\n",
        "- **grad**: Stores the computed gradients\n",
        "- **detach()**: Creates a tensor without gradient tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xm4TlKi8jQ4T"
      },
      "outputs": [],
      "source": [
        "# Create tensors with gradient tracking\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "print(f\"x: {x}, requires_grad: {x.requires_grad}\")\n",
        "print(f\"y: {y}, requires_grad: {y.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtTAztLTjQ4T"
      },
      "outputs": [],
      "source": [
        "# Define a simple function: f(x, y) = x^2 + y^2\n",
        "z = x**2 + y**2\n",
        "print(f\"z = x^2 + y^2 = {z}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTSzHrfljQ4U"
      },
      "outputs": [],
      "source": [
        "# Compute gradients\n",
        "z.backward()\n",
        "\n",
        "print(f\"âˆ‚z/âˆ‚x = 2x = {x.grad}\")\n",
        "print(f\"âˆ‚z/âˆ‚y = 2y = {y.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVmxBLU-jQ4U"
      },
      "source": [
        "Would you expect it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzMCbN5hjQ4U"
      },
      "source": [
        "#### More complex example: linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3fYrQrJjQ4U"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic data\n",
        "x = torch.randn(100, 1)\n",
        "y = 3 * x + 2 + 2.0 * torch.randn(100, 1) + 1.0 * (torch.sin(x)) # Add some noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrQX0_0mjQ4U"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(y=y.numpy()[:, 0], x=x.numpy()[:, 0], label=\"Real\")\n",
        "sns.lineplot(y=(3 * x + 2).numpy()[:, 0], x=x.numpy()[:, 0], color=\"orange\", label=\"Theoretical\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Target\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlulHzU-jQ4U"
      },
      "outputs": [],
      "source": [
        "# Generate random initial parameters (weights)\n",
        "w0 = torch.randn(1, requires_grad=True)\n",
        "w1 = torch.randn(1, requires_grad=True)\n",
        "\n",
        "w0, w1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjFfNng5jQ4U"
      },
      "source": [
        "Make predictions and calculate the loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vA_YT3ztjQ4U"
      },
      "outputs": [],
      "source": [
        "y_pred = w1 * x + w0\n",
        "loss = torch.mean((y_pred - y) ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eD4z2bgSjQ4U"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHAVv83GjQ4U"
      },
      "source": [
        "Backpropagation in one line:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLKWphNojQ4V"
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hnB35d0jQ4V"
      },
      "outputs": [],
      "source": [
        "print(\"âˆ‚L/âˆ‚w0 = \\n\", w0.grad)\n",
        "print(\"âˆ‚L/âˆ‚w1 = \\n\", w1.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koZ6r-jUjQ4V"
      },
      "source": [
        "#### Training our first model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u0EZ9lmjQ4V"
      },
      "outputs": [],
      "source": [
        "n_epochs = 200\n",
        "lr = 0.01\n",
        "w0 = torch.randn(1, requires_grad=True)\n",
        "w1 = torch.randn(1, requires_grad=True)\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    y_pred = w1 * x + w0\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "    # Calculate gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Do a step towards anti-gradient (gradient descent)\n",
        "    w0.data -= lr * w0.grad.data\n",
        "    w1.data -= lr * w1.grad.data\n",
        "\n",
        "    # Zero out the gradients, otherwise PyTorch will sum them up!!!\n",
        "    w0.grad.data.zero_()\n",
        "    w1.grad.data.zero_()\n",
        "\n",
        "    # The rest code is just in order to track progress\n",
        "    if (i+1) % 5 == 0:\n",
        "        clear_output(True)\n",
        "        sns.scatterplot(x=x.data.numpy()[:, 0], y=y.data.numpy()[:, 0], label=\"Real data\")\n",
        "        sns.lineplot(x=x.data.numpy()[:, 0], y=y_pred.data.numpy()[:, 0], label=\"Model\",\n",
        "                    color=\"black\")\n",
        "        sns.lineplot(x=x.numpy()[:, 0], y=(3 * x + 2).numpy()[:, 0], label=\"Theoretical\",\n",
        "                    color=\"orange\")\n",
        "        plt.xlabel(\"Feature\")\n",
        "        plt.ylabel(\"Target\")\n",
        "        plt.xlim(-3.0, 3.0)\n",
        "        plt.ylim(-10, 10)\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Epoch {i + 1} / {n_epochs}\")\n",
        "        print(\"loss = \", loss.data.numpy())\n",
        "        if loss.data.numpy() < 4.25:\n",
        "            print(\"Done!\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NB2eiW5jQ4V"
      },
      "source": [
        "What if we don't zero out the gradients:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-5Sr4iQjQ4V"
      },
      "outputs": [],
      "source": [
        "n_epochs = 200\n",
        "lr = 0.01\n",
        "w0 = torch.randn(1, requires_grad=True)\n",
        "w1 = torch.randn(1, requires_grad=True)\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    y_pred = w1 * x + w0\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = torch.mean((y_pred - y) ** 2)\n",
        "\n",
        "    # Calculate gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Do a step towards anti-gradient (gradient descent)\n",
        "    w0.data -= lr * w0.grad.data\n",
        "    w1.data -= lr * w1.grad.data\n",
        "\n",
        "    # Don't zero out the gradients, so PyTorch will sum them up!!!\n",
        "    # w0.grad.data.zero_()\n",
        "    # w1.grad.data.zero_()\n",
        "\n",
        "    # The rest code is just in order to track progress\n",
        "    if (i+1) % 5 == 0:\n",
        "        clear_output(True)\n",
        "        sns.scatterplot(x=x.data.numpy()[:, 0], y=y.data.numpy()[:, 0], label=\"Real data\")\n",
        "        sns.lineplot(x=x.data.numpy()[:, 0], y=y_pred.data.numpy()[:, 0], label=\"Model\",\n",
        "                    color=\"black\")\n",
        "        sns.lineplot(x=x.numpy()[:, 0], y=(3 * x + 2).numpy()[:, 0], label=\"Theoretical\",\n",
        "                    color=\"orange\")\n",
        "        plt.xlabel(\"Feature\")\n",
        "        plt.ylabel(\"Target\")\n",
        "        plt.xlim(-3.0, 3.0)\n",
        "        plt.ylim(-10, 10)\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Epoch {i + 1} / {n_epochs}\")\n",
        "        print(\"loss = \", loss.data.numpy())\n",
        "        if loss.data.numpy() < 4.25:\n",
        "            print(\"Done!\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9x3BgtpjQ4V"
      },
      "source": [
        "Computational graphs are dynamic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oT97hvhsjQ4V"
      },
      "outputs": [],
      "source": [
        "loss.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaBKs6T5jQ4V"
      },
      "source": [
        "## 4. Neural Networks with nn.Module\n",
        "\n",
        "PyTorch provides the `nn.Module` class as the base for all neural network layers and models.\n",
        "\n",
        "### Key Concepts:\n",
        "- **nn.Module**: Base class for all neural network modules\n",
        "- **Layers**: Building blocks (Linear, Conv2d, LSTM, etc.)\n",
        "- **Sequential**: Container for stacking layers\n",
        "- **Parameters**: Automatically tracked for optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP33ZwSTjQ4W"
      },
      "outputs": [],
      "source": [
        "# Method 1: Using nn.Sequential\n",
        "model_sequential = nn.Sequential(\n",
        "    nn.Linear(10, 20),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 20),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 1)\n",
        ")\n",
        "model_sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xlx9vcLOjQ4W"
      },
      "source": [
        "What is `nn.ReLU()`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__Ne0POnjQ4W"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return torch.max(x, torch.zeros_like(x))\n",
        "\n",
        "x = torch.tensor([-3, -2, -1, 0, 1, 2, 3])\n",
        "sns.lineplot(x=x.numpy(), y=relu(x).numpy())\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"relu(x)\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPV0IvSFjQ4W"
      },
      "outputs": [],
      "source": [
        "# Method 2: Custom nn.Module\n",
        "class MyAwesomeNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Here we can do a lot of things, like:\n",
        "        - Initialize the weights and biases\n",
        "        - Define the activation functions\n",
        "        - Define the number of layers\n",
        "        But now let's just do something simple:\n",
        "        \"\"\"\n",
        "        super(MyAwesomeNet, self).__init__() # Very important!\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        # x here is a tensor of logits (-âˆž, âˆž)\n",
        "        return x\n",
        "\n",
        "model = MyAwesomeNet(input_size=10, hidden_size=20, output_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nKMfgQ9jQ4W"
      },
      "outputs": [],
      "source": [
        "# The number of trainable parameters\n",
        "print(f\"\\nTrainable parameters for sequential model: {sum(p.numel() for p in model_sequential.parameters() if p.requires_grad)}\")\n",
        "print(f\"Trainable parameters for custom model: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMbwqMhfjQ4W"
      },
      "outputs": [],
      "source": [
        "# Using the model\n",
        "# Create dummy input\n",
        "x = torch.randn(5, 10)  # 5 samples, 10 features\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "\n",
        "# Forward pass\n",
        "# We don't need to compute gradients here, so we use torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    output = model_sequential(x)\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Output: {output}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lmeaqZYjQ4W"
      },
      "source": [
        "## 5. Training a Neural Network\n",
        "\n",
        "The most difficult but the most important. Now we need to implement a complete training loop for a neural network.\n",
        "\n",
        "### Training Components:\n",
        "- **Dataset**: Abstract class for data representation\n",
        "- **Data Loader**: Batches data efficiently\n",
        "- **Loss Function**: Measures prediction error\n",
        "- **Optimizer**: Updates model parameters\n",
        "- **Training Loop**: Forward pass, backward pass, parameter updates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhRYd6WmjQ4W"
      },
      "outputs": [],
      "source": [
        "!wget \"https://raw.githubusercontent.com/PickyBinders/ml-ai-summer-school-vib/refs/heads/main/0_dl_pytorch_intro/data/dimers_features.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrW7j-VPjQ4W"
      },
      "outputs": [],
      "source": [
        "# TODO: change the path to the data\n",
        "dimer_data_path = Path(\"dimers_features.csv\")\n",
        "dimer_data = pd.read_csv(dimer_data_path, index_col=0)\n",
        "y = dimer_data[\"physiological\"]\n",
        "print(f\"Class distribution: {y.value_counts()}\")\n",
        "print(f\"Number of dimers: {dimer_data.shape[0]}\")\n",
        "print(f\"Number of columns: {dimer_data.shape[1]}\")\n",
        "dimer_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtzsbEn_jQ4X"
      },
      "source": [
        "For now we will just use a subset of the features and discuss this dataset more later:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWinYPfIjQ4X"
      },
      "outputs": [],
      "source": [
        "features_to_use = [col for col in dimer_data.columns if col.startswith(\"split_\")]\n",
        "X = dimer_data[features_to_use]\n",
        "print(f\"Number of used features: {X.shape[1]}\")\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m86TLkKjQ4X"
      },
      "source": [
        "To train a good model we should always evaluate its performance on a subset of the data that it doesn't use for training. Now we will split the data randomly, but usually it's not that trivial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5u8aBuLjQ4X"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-q4d3AVjQ4X"
      },
      "source": [
        "Normalize the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plu0JYA6jQ4X"
      },
      "outputs": [],
      "source": [
        "x_mean, x_std = X_train.mean(axis=0), X_train.std(axis=0)\n",
        "X_train_scaled = (X_train - x_mean) / x_std\n",
        "X_test_scaled = (X_test - x_mean) / x_std"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0dQSpaejQ4X"
      },
      "source": [
        "#### Create a custom pytorch dataset\n",
        "\n",
        "- Inherit your class from torch `Dataset`\n",
        "- Implement 2 mandatory methods:\n",
        "    * `__getitem__`\n",
        "    * `__len__`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4Ka0WkEjQ4X"
      },
      "outputs": [],
      "source": [
        "class DimerDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.feature_names = features.columns.tolist()\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        features = torch.tensor(self.features.iloc[idx].values, dtype=torch.float32)\n",
        "        target = torch.tensor(self.labels.iloc[idx], dtype=torch.float32)\n",
        "        return features, target\n",
        "\n",
        "train_dataset = DimerDataset(X_train_scaled, y_train)\n",
        "test_dataset = DimerDataset(X_test_scaled, y_test)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNZJhTB3jQ4X"
      },
      "outputs": [],
      "source": [
        "# Create data loaders (efficient batching)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "# Why do we need shuffle=False or True?\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRfhXBoGjQ4X"
      },
      "outputs": [],
      "source": [
        "train_batch = next(iter(train_loader))\n",
        "train_batch[0].shape, train_batch[1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYIWyK6djQ4X"
      },
      "source": [
        "#### Model training\n",
        "\n",
        "Initialize `model`, `criterion` (loss function) and `optimizer` to perform gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWcYRTyujQ4Y"
      },
      "outputs": [],
      "source": [
        "model = MyAwesomeNet(input_size=X_train.shape[1], hidden_size=1280, output_size=2)\n",
        "criterion = nn.CrossEntropyLoss() # Or loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wp7dx3ojQ4Y"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "print(\"=== Training Loop ===\")\n",
        "\n",
        "n_epochs = 100\n",
        "batch_size = 32\n",
        "train_losses = []\n",
        "test_accuracies = []\n",
        "train_accuracies = []\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Mini-batch training\n",
        "    train_predictions = []\n",
        "    train_targets = []\n",
        "    for batch_X, batch_y in train_loader:\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y.to(torch.long))\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        train_predictions.append(predicted.numpy())\n",
        "        train_targets.append(batch_y.numpy())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_predictions = np.concatenate(train_predictions)\n",
        "    train_targets = np.concatenate(train_targets)\n",
        "    train_accuracies.append(accuracy_score(train_targets, train_predictions))\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    test_predictions = []\n",
        "    test_targets = []\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            test_outputs = model(batch_X)\n",
        "            _, predicted = torch.max(test_outputs, 1)\n",
        "            test_predictions.append(predicted.numpy())\n",
        "            test_targets.append(batch_y.numpy())\n",
        "\n",
        "    test_predictions = np.concatenate(test_predictions)\n",
        "    test_targets = np.concatenate(test_targets)\n",
        "    train_losses.append(total_loss / (len(X_train) // batch_size))\n",
        "    test_accuracies.append(accuracy_score(test_targets, test_predictions))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:3d}: Loss = {train_losses[-1]:.4f}, Train Acc = {train_accuracies[-1]:.2f}, Test Acc = {test_accuracies[-1]:.2f}\")\n",
        "\n",
        "print(f\"\\nFinal Test Accuracy: {test_accuracies[-1]:.4f}\")\n",
        "print(f\"Time taken: {time.time() - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I5OgzEJjQ4Y"
      },
      "source": [
        "### Exercise:\n",
        "\n",
        "- What would happen if you don't normalize X?\n",
        "- Try to run training with unnormalized X."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzuXXuOCjQ4Y"
      },
      "outputs": [],
      "source": [
        "# <<<YOUR CODE HERE>>>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGIlPH4-jQ4Y"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(x=range(n_epochs), y=train_losses, label=\"Train\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouFFfrScjQ4Y"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(x=range(n_epochs), y=train_accuracies, label=\"Train\");\n",
        "sns.lineplot(x=range(n_epochs), y=test_accuracies, label=\"Test\");\n",
        "plt.xlabel(\"Epoch\");\n",
        "plt.ylabel(\"Accuracy\");\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdiZ6KvfjQ4Y"
      },
      "source": [
        "## 7. GPU Acceleration\n",
        "\n",
        "PyTorch makes it easy to use GPUs for faster computation.\n",
        "\n",
        "### Key Concepts:\n",
        "- **device**: CPU or GPU\n",
        "- **to()**: Move tensors/models to device\n",
        "- **cuda.is_available()**: Check GPU availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGiOGyyPjQ4Y"
      },
      "outputs": [],
      "source": [
        "model = MyAwesomeNet(input_size=X_train.shape[1], hidden_size=1280, output_size=2)\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss() # Or loss\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwov-U2JjQ4Z"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "print(\"=== Training Loop ===\")\n",
        "\n",
        "n_epochs = 100\n",
        "batch_size = 32\n",
        "train_losses = []\n",
        "test_accuracies = []\n",
        "train_accuracies = []\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Mini-batch training\n",
        "    train_predictions = []\n",
        "    train_targets = []\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        # Put tensors on GPU\n",
        "        batch_X = batch_X.to(device)\n",
        "        batch_y = batch_y.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y.to(torch.long))\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        train_predictions.append(predicted.cpu().numpy())\n",
        "        train_targets.append(batch_y.cpu().numpy())\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    train_predictions = np.concatenate(train_predictions)\n",
        "    train_targets = np.concatenate(train_targets)\n",
        "    train_accuracies.append(accuracy_score(train_targets, train_predictions))\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    test_predictions = []\n",
        "    test_targets = []\n",
        "    with torch.no_grad():\n",
        "        for batch_X, batch_y in test_loader:\n",
        "            batch_X = batch_X.to(device)\n",
        "            batch_y = batch_y.to(device)\n",
        "            test_outputs = model(batch_X)\n",
        "            _, predicted = torch.max(test_outputs, 1)\n",
        "            test_predictions.append(predicted.cpu().numpy())\n",
        "            test_targets.append(batch_y.cpu().numpy())\n",
        "\n",
        "    test_predictions = np.concatenate(test_predictions)\n",
        "    test_targets = np.concatenate(test_targets)\n",
        "    train_losses.append(total_loss / (len(X_train) // batch_size))\n",
        "    test_accuracies.append(accuracy_score(test_targets, test_predictions))\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch {epoch:3d}: Loss = {train_losses[-1]:.4f}, Train Acc = {train_accuracies[-1]:.2f}, Test Acc = {test_accuracies[-1]:.2f}\")\n",
        "\n",
        "print(f\"\\nFinal Test Accuracy: {test_accuracies[-1]:.4f}\")\n",
        "print(f\"Time taken: {time.time() - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUxcRM3DjQ4Z"
      },
      "source": [
        "## 8. Best Practices and Tips\n",
        "\n",
        "### Memory Management:\n",
        "- Use `torch.no_grad()` for inference\n",
        "- Clear gradients with `optimizer.zero_grad()`\n",
        "- Use appropriate batch sizes\n",
        "\n",
        "### Debugging:\n",
        "- Check tensor shapes with `.shape`\n",
        "- Use `print()` or `torch.sum()` for debugging\n",
        "- Verify device placement\n",
        "\n",
        "### Performance:\n",
        "- Use GPU when available\n",
        "\n",
        "### Common Pitfalls:\n",
        "- Forgetting to call `model.train()` and `model.eval()`\n",
        "- Not zeroing gradients\n",
        "- Mixing CPU and GPU tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSaGFX-NjQ4Z"
      },
      "source": [
        "## 9. Summary and Next Steps\n",
        "\n",
        "### What We've Covered:\n",
        "1. **Tensors**: Basic operations and manipulation\n",
        "2. **Autograd**: Automatic differentiation\n",
        "3. **Neural Networks**: Building models with nn.Module\n",
        "4. **Training**: Complete training loops\n",
        "5. **Data Handling**: Datasets and DataLoaders\n",
        "6. **GPU Acceleration**: Using CUDA\n",
        "7. **Best Practices**: Common pitfalls and solutions\n",
        "\n",
        "### Resources:\n",
        "- [PyTorch Official Tutorials](https://pytorch.org/tutorials/)\n",
        "- [PyTorch Documentation](https://pytorch.org/docs/)\n",
        "- [PyTorch Forums](https://discuss.pytorch.org/)\n",
        "- [GitHub Examples](https://github.com/pytorch/examples)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python VIB course",
      "language": "python",
      "name": "protein_dl"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}